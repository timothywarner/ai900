{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Introduction to AI\n## Text and Speech Demos"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Basic Frequency Analysis\n\n#### Load a Text Document"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Use Curl to get a document from GitHub and open it\n!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Moon.txt -o Moon.txt\ndoc1 = open(\"Moon.txt\", \"r\")\n\n# Read the document and print its contents\ndoc1Txt = doc1.read()\nprint(doc1Txt)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Normalize the Text"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from string import punctuation\n\n# remove numeric digits\ntxt = ''.join(c for c in doc1Txt if not c.isdigit())\n\n# remove punctuation and make lower case\ntxt = ''.join(c for c in txt if c not in punctuation).lower()\n\n# print the normalized text\nprint (txt)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Get the Frequency Distribution"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\nimport pandas as pd\nfrom nltk.probability import FreqDist\nnltk.download(\"punkt\")\n\n# Tokenize the text into individual words\nwords = nltk.tokenize.word_tokenize(txt)\n\n# Get the frequency distribution of the words into a data frame\nfdist = FreqDist(words)\ncount_frame = pd.DataFrame(fdist, index =[0]).T\ncount_frame.columns = ['Count']\nprint (count_frame)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Plot the distribution as a pareto chart"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# Sort the data frame by frequency\ncounts = count_frame.sort_values('Count', ascending = False)\n\n# Display the top 60 words as a bar plot\nfig = plt.figure(figsize=(16, 9))\nax = fig.gca()    \ncounts['Count'][:60].plot(kind = 'bar', ax = ax)\nax.set_title('Frequency of the most common words')\nax.set_ylabel('Frequency of word')\nax.set_xlabel('Word')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Remove stop words"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get standard stop words from NLTK\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\n\n# Filter out the stop words\ntxt = ' '.join([word for word in txt.split() if word not in (stopwords.words('english'))])\n\n# Get the frequency distribution of the remaining words\nwords = nltk.tokenize.word_tokenize(txt)\nfdist = FreqDist(words)\ncount_frame = pd.DataFrame(fdist, index =[0]).T\ncount_frame.columns = ['Count']\n\n# Plot the frequency of the top 60 words\ncounts = count_frame.sort_values('Count', ascending = False)\nfig = plt.figure(figsize=(16, 9))\nax = fig.gca()    \ncounts['Count'][:60].plot(kind = 'bar', ax = ax)\nax.set_title('Frequency of the most common words')\nax.set_ylabel('Frequency of word')\nax.set_xlabel('Word')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Term Frequency - Inverse Document Frequency\n#### View the documents"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# remind ourselves of the first document\nprint(doc1Txt)\nprint(\"------------------------------------------------\")\n\n# Get a second document, normalize it, and remove stop words\n!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Gettysburg.txt -o Gettysburg.txt\ndoc2 = open(\"Gettysburg.txt\", \"r\")\ndoc2Txt = doc2.read()\nprint (doc2Txt)\nfrom string import punctuation\ntxt2 = ''.join(c for c in doc2Txt if not c.isdigit())\ntxt2 = ''.join(c for c in txt2 if c not in punctuation).lower()\ntxt2 = ' '.join([word for word in txt2.split() if word not in (stopwords.words('english'))])\n\n\n# and a third\nprint(\"------------------------------------------------\")\n!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Cognitive.txt -o Cognitive.txt\ndoc3 = open(\"Cognitive.txt\", \"r\")\ndoc3Txt = doc3.read()\nprint (doc3Txt)\nfrom string import punctuation\ntxt3 = ''.join(c for c in doc3Txt if not c.isdigit())\ntxt3 = ''.join(c for c in txt3 if c not in punctuation).lower()\ntxt3 = ' '.join([word for word in txt3.split() if word not in (stopwords.words('english'))])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Get TF-IDF Values for the top three words in each document"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# install textblob library and define functions for TF-IDF\n!pip install -U textblob\nimport math\nfrom textblob import TextBlob as tb\n\ndef tf(word, doc):\n    return doc.words.count(word) / len(doc.words)\n\ndef contains(word, docs):\n    return sum(1 for doc in docs if word in doc.words)\n\ndef idf(word, docs):\n    return math.log(len(docs) / (1 + contains(word, docs)))\n\ndef tfidf(word, doc, docs):\n    return tf(word,doc) * idf(word, docs)\n\n\n# Create a collection of documents as textblobs\ndoc1 = tb(txt)\ndoc2 = tb(txt2)\ndoc3 = tb(txt3)\ndocs = [doc1, doc2, doc3]\n\n# Use TF-IDF to get the three most important words from each document\nprint('-----------------------------------------------------------')\nfor i, doc in enumerate(docs):\n    print(\"Top words in document {}\".format(i + 1))\n    scores = {word: tfidf(word, doc, docs) for word in doc.words}\n    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n    for word, score in sorted_words[:3]:\n        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Stemming\n\n#### View frequency of unstemmed words from Kennedy's inauguration speech"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load and print text\n!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Inaugural.txt -o Inaugural.txt\ndoc4 = open(\"Inaugural.txt\", \"r\", encoding=\"utf-16\")\ndoc4Txt = doc4.read()\n\nprint(doc4Txt)\n\n# Normalize and remove stop words\nfrom string import punctuation\ndoc4Txt = ''.join(c for c in doc4Txt if not c.isdigit())\ndoc4Txt = ''.join(c for c in doc4Txt if c not in punctuation).lower()\ndoc4Txt = ' '.join([word for word in doc4Txt.split() if word not in (stopwords.words('english'))])\n\n# Get Frequency distribution\nwords = nltk.tokenize.word_tokenize(doc4Txt)\nfdist = FreqDist(words)\ncount_frame = pd.DataFrame(fdist, index =[0]).T\ncount_frame.columns = ['Count']\n\n# Plot frequency\ncounts = count_frame.sort_values('Count', ascending = False)\nfig = plt.figure(figsize=(16, 9))\nax = fig.gca()    \ncounts['Count'][:60].plot(kind = 'bar', ax = ax)\nax.set_title('Frequency of the most common words')\nax.set_ylabel('Frequency of word')\nax.set_xlabel('Word')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Stem the words using the Porter stemmer"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.stem.porter import PorterStemmer\n\n# Get the word stems\nps = PorterStemmer()\nstems = [ps.stem(word) for word in words]\n\n# Get Frequency distribution\nfdist = FreqDist(stems)\ncount_frame = pd.DataFrame(fdist, index =[0]).T\ncount_frame.columns = ['Count']\n\n# Plot frequency\ncounts = count_frame.sort_values('Count', ascending = False)\nfig = plt.figure(figsize=(16, 9))\nax = fig.gca()    \ncounts['Count'][:60].plot(kind = 'bar', ax = ax)\nax.set_title('Frequency of the most common words')\nax.set_ylabel('Frequency of word')\nax.set_xlabel('Word')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Text Analytics\n#### Create a Text Analytics service in Azure\nhttps://portal.azure.com\n\n#### Get the region-specific URI and Key"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "textAnalyticsURI = 'YOUR_REGION.api.cognitive.microsoft.com'\ntextKey = 'YOUR_KEY'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Analyze the Gettysburg and Cognitive documents"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n\n# Define the request headers.\nheaders = {\n    'Content-Type': 'application/json',\n    'Ocp-Apim-Subscription-Key': textKey,\n    'Accept': 'application/json'\n}\n\n# Define the parameters\nparams = urllib.parse.urlencode({\n})\n\n# Define the request body\nbody = {\n  \"documents\": [\n    {\n        \"language\": \"en\",\n        \"id\": \"1\",\n        \"text\": doc2Txt\n    },\n    {\n        \"language\": \"en\",\n        \"id\": \"2\",\n        \"text\": doc3Txt\n    }\n  ]\n}\n\ntry:\n    # Execute the REST API call and get the response.\n    conn = http.client.HTTPSConnection(textAnalyticsURI)\n    conn.request(\"POST\", \"/text/analytics/v2.0/keyPhrases?%s\" % params, str(body), headers)\n    response = conn.getresponse()\n    data = response.read().decode(\"UTF-8\")\n\n    # 'data' contains the JSON response, which includes a collection of documents.\n    parsed = json.loads(data)\n    for document in parsed['documents']:\n        print(\"Document \" + document[\"id\"] + \" key phrases:\")\n        for phrase in document['keyPhrases']:\n            print(\"  \" + phrase)\n        print(\"---------------------------\")\n    conn.close()\n\nexcept Exception as e:\n    print('Error:')\n    print(e)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Perform sentiment analysis"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "body = {\n  \"documents\": [\n    {\n      \"language\": \"en\",\n      \"id\": \"1\",\n      \"text\": \"Wow! cognitive services are fantastic.\"\n    },\n    {\n      \"language\": \"en\",\n      \"id\": \"2\",\n      \"text\": \"I hate it when computers don't understand me.\"\n    }\n  ]\n}\n\n\ntry:\n    conn = http.client.HTTPSConnection(textAnalyticsURI)\n    conn.request(\"POST\", \"/text/analytics/v2.0/sentiment?%s\" % params, str(body), headers)\n    response = conn.getresponse()\n    data = response.read().decode(\"UTF-8\")\n    parsed = json.loads(data)\n    \n    # Get the numeric score for each document\n    for document in parsed['documents']:\n        sentiment = \"negative\"\n        \n        # if it's more than 0.5, consider the sentiment to be positive.\n        if document[\"score\"] >= 0.5:\n            sentiment = \"positive\"\n        print(\"Document:\" + document[\"id\"] + \" = \" + sentiment)\n    conn.close()\n    \nexcept Exception as e:\n    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Speech\n#### Create a Speech API service\nhttps://portal.azure.com\n#### Get the service key"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "speechKey = 'YOUR_KEY'\nspeech_region = 'YOUR_REGION'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Get the audio input\nWe'll use a WAV file of captured audio containing the speech we want to transcribe"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import IPython\n\n# Download the audio file\n!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/RainSpain.wav -o RainSpain.wav\n    \n# Play the audio\nIPython.display.Audio('RainSpain.wav', autoplay=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Convert speech to text"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import IPython\nimport requests, json\n\nwith open(\"RainSpain.wav\", mode=\"rb\") as audio_file:\n        audio_data =  audio_file.read()\n        \n# The Speech API requires an access token (valid for 10 mins)\napiEndPoint = \"https://\" + speech_region + \".api.cognitive.microsoft.com/sts/v1.0/issueToken\"\napiKey = speechKey\nheaders = {\"Ocp-Apim-Subscription-Key\": apiKey}\n\n# Use the API key to request an access token\nresponse = requests.post(apiEndPoint, headers=headers)\naccesstoken = str(response.text)\n\n# Now that we have a token, we can set up the request\nspeechToTextEndPoint = \"https://\" + speech_region + \".stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1\"\nheaders = {\"Content-type\": \"audio/wav; codec=audio/pcm; samplerate=16000\", \n           \"Authorization\": \"Bearer \" + accesstoken}\nparams = {\"language\":\"en-US\"}\nbody = audio_data\n\n# Connect to server, post the request, and get the result\nresponse = requests.post(speechToTextEndPoint,data=body, params=params, headers=headers)\nresult = str(response.text)\nprint(json.loads(result)['DisplayText'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Convert text to speech"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import IPython\nimport requests, json\nfrom xml.etree import ElementTree\n\n# Get the input text\nmyText = input('What would you like me to say?: \\n')\n        \n# The Speech API requires an access token (valid for 10 mins)\napiEndPoint = \"https://\" + speech_region + \".api.cognitive.microsoft.com/sts/v1.0/issueToken\"\napiKey = speechKey\nheaders = {\"Ocp-Apim-Subscription-Key\": apiKey}\n\n# Use the API key to request an access token\nresponse = requests.post(apiEndPoint, headers=headers)\naccesstoken = str(response.text)\n\n# Now that we have a token, we can set up the request\ntextToSpeechEndPoint = \"https://\" + speech_region + \".tts.speech.microsoft.com/cognitiveservices/v1\"\nheaders = {\"Content-type\": \"application/ssml+xml\",\n           'X-Microsoft-OutputFormat': 'riff-16khz-16bit-mono-pcm',\n           'User-Agent': 'speech',\n           \"Authorization\": \"Bearer \" + accesstoken}\n\n# The request body is XML\nbody = \"<speak version='1.0' xml:lang='en-US'>\\\n          <voice xml:lang='en-US'\\\n                 xml:gender='Female'\\\n                 name='Microsoft Server Speech Text to Speech Voice (en-US, ZiraRUS)'>\" + myText + \"</voice>\\\n        </speak>\"\n\n\n# Connect to server, post the request, and get the result\nresponse = requests.post(textToSpeechEndPoint,data=body, headers=headers)\n\n#Play the audio\nIPython.display.Audio(response.content, autoplay=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "### Translation\n\n#### Create a Microsoft Translator Text  Service\nhttps://portal.azure.com\n\n#### Get the service key"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "transTextKey = \"YOUR_KEY_HERE\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Translate Text"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests, http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\nfrom xml.etree import ElementTree\n\n\ntextToTranslate = input('Please enter some text: \\n')\nfromLangCode = input('What language is this?: \\n') \ntoLangCode = input('To what language would you like it translated?: \\n') \n\ntry:\n    # Connect to server to get the Access Token\n    apiKey = transTextKey\n    params = \"\"\n    headers = {\"Ocp-Apim-Subscription-Key\": apiKey}\n    AccessTokenHost = \"api.cognitive.microsoft.com\"\n    path = \"/sts/v1.0/issueToken\"\n\n    conn = http.client.HTTPSConnection(AccessTokenHost)\n    conn.request(\"POST\", path, params, headers)\n    response = conn.getresponse()\n    data = response.read()\n    conn.close()\n    accesstoken = \"Bearer \" + data.decode(\"UTF-8\")\n\n\n    # Define the request headers.\n    headers = {\n        'Authorization': accesstoken\n    }\n\n    # Define the parameters\n    params = urllib.parse.urlencode({\n        \"text\": textToTranslate,\n        \"to\": toLangCode,\n        \"from\": fromLangCode\n    })\n\n    # Execute the REST API call and get the response.\n    conn = http.client.HTTPSConnection(\"api.microsofttranslator.com\")\n    conn.request(\"GET\", \"/V2/Http.svc/Translate?%s\" % params, \"{body}\", headers)\n    response = conn.getresponse()\n    data = response.read()\n    translation = ElementTree.fromstring(data.decode(\"utf-8\"))\n    print (translation.text)\n\n    conn.close()\n    \nexcept Exception as e:\n    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Language Understanding Intelligence Service (LUIS)\n\n#### Provision LUIS\nhttps://portal.azure.com\n\n#### Create a LUIS App\nhttps://www.luis.ai/\n\nHome automation app with a *Light* entity and the following intents:\n- Light On\n- Light Off\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport json \n\n# Set up API configuration\nendpointUrl = \"YOUR_LUIS_ENDPOINT\"\n\n# prompt for a command\ncommand = input('Please enter a command: \\n')\n\n# Call the LUIS service and get the JSON response\nendpoint = endpointUrl + command.replace(\" \",\"+\")\nresponse = requests.get(endpoint)\ndata = json.loads(response.content.decode(\"UTF-8\"))\n\n# Identify the top scoring intent\nintent = data[\"topScoringIntent\"][\"intent\"]\nif (intent == \"Light On\"):\n    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.jpg'\nelif (intent == \"Light Off\"):\n    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.jpg'\nelse:\n    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Dunno.jpg'\n\n# Get the appropriate image and show it\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\nimshow(img)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}