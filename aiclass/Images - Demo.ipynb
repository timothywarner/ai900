{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Computer Vision\n\n## Manipulating Images\n\n\n### Load an Image\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image\nimport numpy as np\nimport skimage.color as sc\n\n!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme2.jpg -o img.jpg\n\ni = np.array(Image.open('img.jpg'))\nimshow(i)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Examine Numerical Properties of the Image\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "type(i)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "i.dtype",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "i.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "i_mono = sc.rgb2gray(i)\nimshow(i_mono, cmap='gray')\ni_mono.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### View Pixel Value Distributions\nPlot a histogram"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def im_hist(img):\n    import matplotlib.pyplot as plt    \n    fig = plt.figure(figsize=(8, 6))\n    fig.clf()\n    ax = fig.gca()    \n    ax.hist(img.flatten(), bins = 256)\n    plt.show()\n\nim_hist(i_mono)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Plot a cumulative histogram"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def im_cdf(img):\n    import matplotlib.pyplot as plt    \n    fig = plt.figure(figsize=(8, 6))\n    fig.clf()\n    ax = fig.gca()    \n    ax.hist(img.flatten(), bins = 256, cumulative=True)\n    plt.show()\n    \nim_cdf(i_mono)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Equalize the Image\nUse skimage library to equalize the image.  "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from skimage import exposure\n\ni_eq = exposure.equalize_hist(i_mono)\nimshow(i_eq, cmap='gray')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "View the histogram and CDF plots:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "im_hist(i_eq)\nim_cdf(i_eq)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Denoising with Filters\n\n### Add Noise"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import skimage\ni_n = skimage.util.random_noise(i_eq)\nimshow(i_n, cmap=\"gray\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Use a Gaussian Filter"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def gauss_filter(im, sigma = 10):\n    from scipy.ndimage.filters import gaussian_filter as gf\n    import numpy as np\n    return gf(im, sigma = sigma)   \ni_g = gauss_filter(i_n)\nimshow(i_g, cmap=\"gray\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Use a Median Filter\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def med_filter(im, size = 10):\n    from scipy.ndimage.filters import median_filter as mf\n    import numpy as np\n    return mf(im, size = size)     \ni_m = med_filter(i_n)\nimshow(i_m, cmap=\"gray\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Extract Features\n\n#### Sobel Edge Detection\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def edge_sobel(image):\n    from scipy import ndimage\n    import skimage.color as sc\n    import numpy as np\n    image = sc.rgb2gray(image) # Convert color image to gray scale\n    dx = ndimage.sobel(image, 1)  # horizontal derivative\n    dy = ndimage.sobel(image, 0)  # vertical derivative\n    mag = np.hypot(dx, dy)  # magnitude\n    mag *= 255.0 / np.amax(mag)  # normalize (Q&D)\n    mag = mag.astype(np.uint8)\n    return mag\n\ni_edge = edge_sobel(i_m)\nimshow(i_edge, cmap=\"gray\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Harris Corner Detection\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def corner_harr(im, min_distance = 10):\n    from skimage.feature import corner_harris, corner_peaks\n    mag = corner_harris(im)\n    return corner_peaks(mag, min_distance = min_distance)\n\nharris = corner_harr(i_eq, 10)\n\n\ndef plot_harris(im, harris, markersize = 20, color = 'red'):\n    import matplotlib.pyplot as plt\n    import numpy as np\n    fig = plt.figure(figsize=(6, 6))\n    fig.clf()\n    ax = fig.gca()    \n    ax.imshow(np.array(im).astype(float), cmap=\"gray\")\n    ax.plot(harris[:, 1], harris[:, 0], 'r+', color = color, markersize=markersize)\n    return 'Done'  \n\nplot_harris(i_eq, harris)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The corner detection algorithm has identified the eyes in the image."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Image Classification\n\n### Custom Vision API\nhttps://www.customvision.ai/projects"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Install the Custom Vision SDK\n! pip install azure-cognitiveservices-vision-customvision",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "PREDICTION_KEY = 'YOUR_PREDICTION_KEY'\nENDPOINT='https://YOUR_REGION.api.cognitive.microsoft.com'\nPROJECT_ID = 'YOUR_PROJECT_ID'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n%matplotlib inline\n\n# Use two test images\ntest_img1_url = 'http://www.pachd.com/free-images/food-images/apple-01.jpg'\ntest_img2_url = 'http://www.pachd.com/free-images/food-images/carrot-02.jpg'\n\n# Create an instance of the prediction service\npredictor = CustomVisionPredictionClient(PREDICTION_KEY, endpoint=ENDPOINT)\n\n\n# Get a prediction for image 1\nresult1 = predictor.predict_image_url(PROJECT_ID, url=test_img1_url)\n# The results include a prediction for each tag, in descending order of probability - so we'll get the first one\nprediction1 = result1.predictions[0].tag_name + \": {0:.2f}%\".format(result1.predictions[0].probability * 100)\n\n# Get a prediction for image 2\nresult2 = predictor.predict_image_url(PROJECT_ID, url=test_img2_url)\nprediction2 = result2.predictions[0].tag_name + \": {0:.2f}%\".format(result2.predictions[0].probability * 100)\n    \n# Download the images so we can show them\nresponse = requests.get(test_img1_url)\nimg1 = Image.open(BytesIO(response.content))\n\nresponse = requests.get(test_img2_url)\nimg2 = Image.open(BytesIO(response.content))\n\n# Create a figure\nfig = plt.figure(figsize=(16, 8))\n\n# Subplot for first image and its predicted class\na=fig.add_subplot(1,2,1)\nimgplot = plt.imshow(img1)\na.set_title(prediction1)\n\n# Subplot for second image and its predicted class\na=fig.add_subplot(1,2,2)\nimgplot = plt.imshow(img2)\na.set_title(prediction2)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Use the Computer Vision API\nhttps://portal.azure.com.\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "visionURI = 'YOUR_REGION.api.cognitive.microsoft.com'\nvisionKey = 'YOUR_KEY'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Get An Image from a URL\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nimg_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme2.jpg'\n\n# Get the image and show it\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\nimshow(img)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Use the Computer Vision API to Get Image Features\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_image_features(img_url):\n    import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n\n    headers = {\n        # Request headers.\n        'Content-Type': 'application/json',\n        'Ocp-Apim-Subscription-Key': visionKey,\n    }\n\n    params = urllib.parse.urlencode({\n        # Request parameters. All of them are optional.\n        'visualFeatures': 'Categories,Description,Color',\n        'language': 'en',\n    })\n\n    body = \"{'url':'\" + img_url + \"'}\"\n\n    try:\n        # Execute the REST API call and get the response.\n        conn = http.client.HTTPSConnection(visionURI)\n        conn.request(\"POST\", \"/vision/v1.0/analyze?%s\" % params, body, headers)\n        response = conn.getresponse()\n        data = response.read().decode(\"UTF-8\")\n\n        # 'data' contains the JSON response.\n        parsed = json.loads(data)\n        if response is not None:\n            return parsed\n        conn.close()\n\n\n    except Exception as e:\n        print('Error:')\n        print(e)\n        \njsonData = get_image_features(img_url)\ndesc = jsonData['description']['captions'][0]['text']\nprint(desc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Get the full response"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# View the full details returned\nimport http.client, urllib.request, urllib.parse, urllib.error, base64, json\nprint (json.dumps(jsonData, sort_keys=True, indent=2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try with a different image:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/soccer.jpg'\n\n# Get the image and show it\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\nimshow(img)\njsonData = get_image_features(img_url)\ndesc = jsonData['description']['captions'][0]['text']\nprint(desc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Using the Face API\n\n\n### Create a Face API Service\nhttps://portal.azure.com.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "faceURI = \"https://YOUR_REGION.api.cognitive.microsoft.com/face/v1.0/\"\nfaceKey = \"YOUR_KEY\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Install the Face SDK package. This makes it easier to work with.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install cognitive_face\n!pip install pillow",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Detect a face in an image:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport requests\nfrom io import BytesIO\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image, ImageDraw\nimport cognitive_face as CF\n\n# Set URI and Key\nCF.Key.set(faceKey)\nCF.BaseUrl.set(faceURI)\n\n# Detect faces in an image\nimg_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme1.jpg'\nresult = CF.face.detect(img_url)\n\n# Get the ID of the first face detected\nface1 = result[0]['faceId']\nprint (\"Face 1:\" + face1)\n\n# Get the image\nresponse = requests.get(img_url)\nimg = Image.open(BytesIO(response.content))\n\n# Add rectangles for each face found\ncolor=\"blue\"\nif result is not None:\n    draw = ImageDraw.Draw(img) \n    for currFace in result:\n        faceRectangle = currFace['faceRectangle']\n        left = faceRectangle['left']\n        top = faceRectangle['top']\n        width = faceRectangle['width']\n        height = faceRectangle['height']\n        draw.line([(left,top),(left+width,top)],fill=color, width=5)\n        draw.line([(left+width,top),(left+width,top+height)],fill=color , width=5)\n        draw.line([(left+width,top+height),(left, top+height)],fill=color , width=5)\n        draw.line([(left,top+height),(left, top)],fill=color , width=5)\n\n# show the image\nimshow(img)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Compare with another image"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the image to compare\nimg2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme2.jpg'\nresponse2 = requests.get(img2_url)\nimg2 = Image.open(BytesIO(response2.content))\n\n# Detect faces in a comparison image\nresult2 = CF.face.detect(img2_url)\n\n# Assume the first face is the one we want to compare\nface2 = result2[0]['faceId']\nprint (\"Face 2:\" + face2)\n\ndef verify_face(face1, face2):\n    # By default, assume the match is unverified\n    verified = \"Not Verified\"\n    color=\"red\"\n\n    if result2 is not None:\n        # compare the comparison face to the original one we retrieved previously\n        verify = CF.face.verify(face1, face2)\n\n        # if there's a match, set verified and change color to green\n        if verify['isIdentical'] == True:\n            verified = \"Verified\"\n            color=\"lightgreen\"\n\n        # Display the second face with a red rectange if unverified, or green if verified\n        draw = ImageDraw.Draw(img2) \n        for currFace in result2:\n            faceRectangle = currFace['faceRectangle']\n            left = faceRectangle['left']\n            top = faceRectangle['top']\n            width = faceRectangle['width']\n            height = faceRectangle['height']\n            draw.line([(left,top),(left+width,top)] , fill=color, width=5)\n            draw.line([(left+width,top),(left+width,top+height)] , fill=color, width=5)\n            draw.line([(left+width,top+height),(left, top+height)] , fill=color, width=5)\n            draw.line([(left,top+height),(left, top)] , fill=color, width=5)\n\n    # show the image\n    imshow(img2)\n\n    # Display verification status and confidence level\n    print(verified)\n    print (\"Confidence Level: \" + str(verify['confidence']))\n\nverify_face(face1, face2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And another?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the image to compare\nimg2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/graeme3.jpg'\nresponse2 = requests.get(img2_url)\nimg2 = Image.open(BytesIO(response2.content))\n\n# Detect faces in a comparison image\nresult2 = CF.face.detect(img2_url)\n\n# Assume the first face is the one we want to compare\nface2 = result2[0]['faceId']\nprint (\"Face 2:\" + face2)\n\nverify_face(face1, face2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And another?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the image to compare\nimg2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/satya.jpg'\nresponse2 = requests.get(img2_url)\nimg2 = Image.open(BytesIO(response2.content))\n\n# Detect faces in a comparison image\nresult2 = CF.face.detect(img2_url)\n\n# Assume the first face is the one we want to compare\nface2 = result2[0]['faceId']\nprint (\"Face 2:\" + face2)\n\nverify_face(face1, face2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "No match!"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}